Namespace(seed=42, GPU_to_use=0, epochs=80, batch_size=128, lr=0.0005, lr_decay=200, gamma=0.5, training_samples=0, test_samples=0, prediction_steps=10, encoder_hidden=256, decoder_hidden=256, encoder='cnn', decoder='mlp', prior=1, edge_types=2, dont_use_encoder=False, lr_z=0.1, global_temp=False, load_temperatures=False, alpha=2, num_cats=3, unobserved=0, model_unobserved=0, dont_shuffle_unobserved=False, teacher_forcing=0, suffix='netsim', timesteps=200, num_atoms=15, dims=1, datadir='./data', save_folder='cnn_netsim', expername='', sym_save_folder='../logs', load_folder='', test_time_adapt=False, lr_logits=0.01, num_tta_steps=100, dont_skip_first=True, temp=0.5, hard=False, no_validate=True, no_cuda=False, var=5e-07, encoder_dropout=0.0, decoder_dropout=0.0, no_factor=False, test=False, device=device(type='cuda', index=0), cuda=True, factor=True, validate=False, shuffle_unobserved=True, skip_first=False, use_encoder=True, time='2025-05-21T14:44:26.907404', num_GPU=1, batch_size_multiGPU=128, log_path='cnn_netsim\\2025-05-21T14.44.26.907404')
Using GPU #0
DataParallel(
  (module): CNNEncoder(
    (cnn): CNN(
      (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv1): Conv1d(2, 256, kernel_size=(5,), stride=(1,))
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,))
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_predict): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (conv_attention): Conv1d(256, 1, kernel_size=(1,), stride=(1,))
    )
    (mlp1): MLP(
      (fc1): Linear(in_features=256, out_features=256, bias=True)
      (fc2): Linear(in_features=256, out_features=256, bias=True)
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (mlp2): MLP(
      (fc1): Linear(in_features=256, out_features=256, bias=True)
      (fc2): Linear(in_features=256, out_features=256, bias=True)
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (mlp3): MLP(
      (fc1): Linear(in_features=768, out_features=256, bias=True)
      (fc2): Linear(in_features=256, out_features=256, bias=True)
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (fc_out): Linear(in_features=256, out_features=2, bias=True)
  )
)
DataParallel(
  (module): MLPDecoder(
    (msg_fc1): ModuleList(
      (0-1): 2 x Linear(in_features=2, out_features=256, bias=True)
    )
    (msg_fc2): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
    (out_fc1): Linear(in_features=257, out_features=256, bias=True)
    (out_fc2): Linear(in_features=256, out_features=256, bias=True)
    (out_fc3): Linear(in_features=256, out_features=1, bias=True)
  )
)
0 train	 	loss_kl -7.1391463280 	acc 0.5019047619 	auroc 0.5303703704 	loss_nll 18651752.0000000000 	loss_mse 0.0937273949 	loss 18651744.0000000000 	inference time 0.3321361542 	time: 5.4786s 	
1 train	 	loss_kl -4.9214682579 	acc 0.4980952381 	auroc 0.5300347222 	loss_nll 27208056.0000000000 	loss_mse 0.1367238909 	loss 27208052.0000000000 	inference time 0.2690849304 	time: 5.2886s 	
2 train	 	loss_kl -3.2741382122 	acc 0.5257142857 	auroc 0.5258796296 	loss_nll 17336016.0000000000 	loss_mse 0.0871156529 	loss 17336012.0000000000 	inference time 0.2533767223 	time: 5.3186s 	
3 train	 	loss_kl -2.8220520020 	acc 0.5028571429 	auroc 0.5247106481 	loss_nll 21606794.0000000000 	loss_mse 0.1085768491 	loss 21606792.0000000000 	inference time 0.3976755142 	time: 5.4607s 	
4 train	 	loss_kl -2.5201921463 	acc 0.4990476190 	auroc 0.5281539352 	loss_nll 18378754.0000000000 	loss_mse 0.0923555419 	loss 18378752.0000000000 	inference time 0.2687425613 	time: 5.2281s 	
5 train	 	loss_kl -2.4064064026 	acc 0.5133333333 	auroc 0.5373206019 	loss_nll 18364120.0000000000 	loss_mse 0.0922819972 	loss 18364118.0000000000 	inference time 0.3958303928 	time: 5.4139s 	
6 train	 	loss_kl -2.1236169338 	acc 0.5247619048 	auroc 0.5501793981 	loss_nll 16209794.0000000000 	loss_mse 0.0814562440 	loss 16209792.0000000000 	inference time 0.3955664635 	time: 5.4205s 	
7 train	 	loss_kl -1.8653995991 	acc 0.5285714286 	auroc 0.5446932870 	loss_nll 14098757.0000000000 	loss_mse 0.0708480254 	loss 14098755.0000000000 	inference time 0.2853577137 	time: 5.4033s 	
8 train	 	loss_kl -1.6974985600 	acc 0.5285714286 	auroc 0.5400405093 	loss_nll 13936281.0000000000 	loss_mse 0.0700315535 	loss 13936279.0000000000 	inference time 0.3799936771 	time: 5.4017s 	
9 train	 	loss_kl -1.6293889284 	acc 0.5304761905 	auroc 0.5459432870 	loss_nll 13610348.0000000000 	loss_mse 0.0683936998 	loss 13610346.0000000000 	inference time 0.4077548981 	time: 5.3985s 	
10 train	 	loss_kl -1.6337026358 	acc 0.5304761905 	auroc 0.5504687500 	loss_nll 13084907.0000000000 	loss_mse 0.0657533035 	loss 13084905.0000000000 	inference time 0.2845194340 	time: 5.3774s 	
11 train	 	loss_kl -1.6262416840 	acc 0.5304761905 	auroc 0.5514814815 	loss_nll 12967230.0000000000 	loss_mse 0.0651619509 	loss 12967228.0000000000 	inference time 0.3013739586 	time: 5.3564s 	
12 train	 	loss_kl -1.5852837563 	acc 0.5295238095 	auroc 0.5515277778 	loss_nll 13481623.0000000000 	loss_mse 0.0677468404 	loss 13481621.0000000000 	inference time 0.3032853603 	time: 5.4026s 	
13 train	 	loss_kl -1.5759863853 	acc 0.5219047619 	auroc 0.5599016204 	loss_nll 13617500.0000000000 	loss_mse 0.0684296489 	loss 13617498.0000000000 	inference time 0.2532312870 	time: 5.3218s 	
14 train	 	loss_kl -1.4489966631 	acc 0.5152380952 	auroc 0.5633796296 	loss_nll 13090262.0000000000 	loss_mse 0.0657802075 	loss 13090261.0000000000 	inference time 0.3004934788 	time: 5.2916s 	
15 train	 	loss_kl -1.3604174852 	acc 0.5000000000 	auroc 0.5654224537 	loss_nll 13091474.0000000000 	loss_mse 0.0657862946 	loss 13091473.0000000000 	inference time 0.3472929001 	time: 5.3711s 	
16 train	 	loss_kl -1.2833358049 	acc 0.5123809524 	auroc 0.5639004630 	loss_nll 13135157.0000000000 	loss_mse 0.0660058111 	loss 13135156.0000000000 	inference time 0.3949160576 	time: 5.5106s 	
17 train	 	loss_kl -1.2912341356 	acc 0.5238095238 	auroc 0.5665682870 	loss_nll 12945993.0000000000 	loss_mse 0.0650552362 	loss 12945992.0000000000 	inference time 0.3018052578 	time: 5.3284s 	
18 train	 	loss_kl -1.2138069868 	acc 0.5190476190 	auroc 0.5656655093 	loss_nll 12933566.0000000000 	loss_mse 0.0649927929 	loss 12933565.0000000000 	inference time 0.2533974648 	time: 5.3051s 	
19 train	 	loss_kl -1.0380451679 	acc 0.5209523810 	auroc 0.5702430556 	loss_nll 13069483.0000000000 	loss_mse 0.0656757876 	loss 13069482.0000000000 	inference time 0.2854454517 	time: 5.3279s 	
20 train	 	loss_kl -0.9875186682 	acc 0.5390476190 	auroc 0.5720312500 	loss_nll 12945160.0000000000 	loss_mse 0.0650510490 	loss 12945159.0000000000 	inference time 0.3479826450 	time: 5.4294s 	
21 train	 	loss_kl -1.0872330666 	acc 0.5476190476 	auroc 0.5658912037 	loss_nll 12896088.0000000000 	loss_mse 0.0648044646 	loss 12896087.0000000000 	inference time 0.2257905006 	time: 5.2793s 	
22 train	 	loss_kl -1.0721765757 	acc 0.5590476190 	auroc 0.5659201389 	loss_nll 12947865.0000000000 	loss_mse 0.0650646463 	loss 12947864.0000000000 	inference time 0.2702190876 	time: 5.3349s 	
23 train	 	loss_kl -1.0908399820 	acc 0.5571428571 	auroc 0.5634027778 	loss_nll 12889355.0000000000 	loss_mse 0.0647706240 	loss 12889354.0000000000 	inference time 0.3162143230 	time: 5.4341s 	
24 train	 	loss_kl -1.1142530441 	acc 0.5504761905 	auroc 0.5631365741 	loss_nll 12832511.0000000000 	loss_mse 0.0644849762 	loss 12832510.0000000000 	inference time 0.2687046528 	time: 5.2737s 	
25 train	 	loss_kl -1.0781053305 	acc 0.5485714286 	auroc 0.5628182870 	loss_nll 12844698.0000000000 	loss_mse 0.0645462200 	loss 12844697.0000000000 	inference time 0.3947694302 	time: 5.4601s 	
26 train	 	loss_kl -1.0615222454 	acc 0.5342857143 	auroc 0.5597337963 	loss_nll 12864495.0000000000 	loss_mse 0.0646457002 	loss 12864494.0000000000 	inference time 0.3324241638 	time: 5.4318s 	
27 train	 	loss_kl -1.1463959217 	acc 0.5352380952 	auroc 0.5551736111 	loss_nll 12841529.0000000000 	loss_mse 0.0645302832 	loss 12841528.0000000000 	inference time 0.3804733753 	time: 5.4188s 	
28 train	 	loss_kl -1.0782999992 	acc 0.5390476190 	auroc 0.5529803241 	loss_nll 12834886.0000000000 	loss_mse 0.0644969121 	loss 12834885.0000000000 	inference time 0.3569965363 	time: 5.4151s 	
29 train	 	loss_kl -0.9901792407 	acc 0.5361904762 	auroc 0.5537731481 	loss_nll 12851850.0000000000 	loss_mse 0.0645821542 	loss 12851849.0000000000 	inference time 0.3482375145 	time: 5.3681s 	
30 train	 	loss_kl -0.9082725048 	acc 0.5323809524 	auroc 0.5537731481 	loss_nll 12847854.0000000000 	loss_mse 0.0645620823 	loss 12847853.0000000000 	inference time 0.3492424488 	time: 5.4142s 	
31 train	 	loss_kl -0.8772031069 	acc 0.5209523810 	auroc 0.5567824074 	loss_nll 12833101.0000000000 	loss_mse 0.0644879490 	loss 12833100.0000000000 	inference time 0.3625335693 	time: 5.3670s 	
32 train	 	loss_kl -0.9376916885 	acc 0.5304761905 	auroc 0.5564641204 	loss_nll 12837771.0000000000 	loss_mse 0.0645114034 	loss 12837770.0000000000 	inference time 0.3790097237 	time: 5.4178s 	
33 train	 	loss_kl -0.9576123357 	acc 0.5304761905 	auroc 0.5575868056 	loss_nll 12841508.0000000000 	loss_mse 0.0645301938 	loss 12841507.0000000000 	inference time 0.2846617699 	time: 5.3364s 	
34 train	 	loss_kl -0.8156355619 	acc 0.5238095238 	auroc 0.5582118056 	loss_nll 12829065.0000000000 	loss_mse 0.0644676685 	loss 12829064.0000000000 	inference time 0.2999861240 	time: 5.3487s 	
35 train	 	loss_kl -0.7142396569 	acc 0.5171428571 	auroc 0.5590451389 	loss_nll 12825093.0000000000 	loss_mse 0.0644477010 	loss 12825092.0000000000 	inference time 0.2852666378 	time: 5.3224s 	
36 train	 	loss_kl -0.6935319901 	acc 0.5180952381 	auroc 0.5594502315 	loss_nll 12830618.0000000000 	loss_mse 0.0644754693 	loss 12830617.0000000000 	inference time 0.2400903702 	time: 5.3453s 	
37 train	 	loss_kl -0.6747790575 	acc 0.5200000000 	auroc 0.5598148148 	loss_nll 12825290.0000000000 	loss_mse 0.0644486919 	loss 12825289.0000000000 	inference time 0.2545232773 	time: 5.3140s 	
38 train	 	loss_kl -0.6534045339 	acc 0.5200000000 	auroc 0.5605439815 	loss_nll 12819828.0000000000 	loss_mse 0.0644212440 	loss 12819827.0000000000 	inference time 0.2516963482 	time: 5.3378s 	
39 train	 	loss_kl -0.6591858268 	acc 0.5209523810 	auroc 0.5608506944 	loss_nll 12823950.0000000000 	loss_mse 0.0644419491 	loss 12823949.0000000000 	inference time 0.3072030544 	time: 5.3061s 	
40 train	 	loss_kl -0.6589667201 	acc 0.5219047619 	auroc 0.5624652778 	loss_nll 12821673.0000000000 	loss_mse 0.0644305199 	loss 12821672.0000000000 	inference time 0.3005678654 	time: 5.3196s 	
41 train	 	loss_kl -0.6758235693 	acc 0.5228571429 	auroc 0.5631712963 	loss_nll 12816001.0000000000 	loss_mse 0.0644020140 	loss 12816000.0000000000 	inference time 0.2692859173 	time: 5.2776s 	
42 train	 	loss_kl -0.6636673212 	acc 0.5257142857 	auroc 0.5623206019 	loss_nll 12817641.0000000000 	loss_mse 0.0644102618 	loss 12817640.0000000000 	inference time 0.2557089329 	time: 5.2760s 	
43 train	 	loss_kl -0.6645457745 	acc 0.5209523810 	auroc 0.5627719907 	loss_nll 12819589.0000000000 	loss_mse 0.0644200444 	loss 12819588.0000000000 	inference time 0.3975615501 	time: 5.4826s 	
44 train	 	loss_kl -0.6471716762 	acc 0.5219047619 	auroc 0.5622164352 	loss_nll 12813032.0000000000 	loss_mse 0.0643870980 	loss 12813031.0000000000 	inference time 0.3492207527 	time: 5.4646s 	
45 train	 	loss_kl -0.6058707237 	acc 0.5238095238 	auroc 0.5616493056 	loss_nll 12814166.0000000000 	loss_mse 0.0643927902 	loss 12814165.0000000000 	inference time 0.2844042778 	time: 5.2763s 	
46 train	 	loss_kl -0.5845110416 	acc 0.5228571429 	auroc 0.5618981481 	loss_nll 12815606.0000000000 	loss_mse 0.0644000247 	loss 12815605.0000000000 	inference time 0.4015176296 	time: 5.4137s 	
47 train	 	loss_kl -0.5774370432 	acc 0.5200000000 	auroc 0.5626851852 	loss_nll 12812328.0000000000 	loss_mse 0.0643835515 	loss 12812327.0000000000 	inference time 0.3950626850 	time: 5.4496s 	
48 train	 	loss_kl -0.5719347000 	acc 0.5200000000 	auroc 0.5624768519 	loss_nll 12813189.0000000000 	loss_mse 0.0643878877 	loss 12813188.0000000000 	inference time 0.3498950005 	time: 5.4343s 	
49 train	 	loss_kl -0.5615521669 	acc 0.5200000000 	auroc 0.5627256944 	loss_nll 12813544.0000000000 	loss_mse 0.0643896684 	loss 12813543.0000000000 	inference time 0.3015787601 	time: 5.4499s 	
50 train	 	loss_kl -0.5360231996 	acc 0.5200000000 	auroc 0.5636111111 	loss_nll 12808968.0000000000 	loss_mse 0.0643666759 	loss 12808967.0000000000 	inference time 0.3111317158 	time: 5.3527s 	
51 train	 	loss_kl -0.5155943632 	acc 0.5180952381 	auroc 0.5630208333 	loss_nll 12809858.0000000000 	loss_mse 0.0643711463 	loss 12809857.0000000000 	inference time 0.3996014595 	time: 5.4047s 	
52 train	 	loss_kl -0.5134208202 	acc 0.5180952381 	auroc 0.5624537037 	loss_nll 12808478.0000000000 	loss_mse 0.0643642098 	loss 12808477.0000000000 	inference time 0.3641798496 	time: 5.3854s 	
53 train	 	loss_kl -0.5128145218 	acc 0.5171428571 	auroc 0.5624189815 	loss_nll 12805593.0000000000 	loss_mse 0.0643497109 	loss 12805592.0000000000 	inference time 0.4107918739 	time: 5.3697s 	
54 train	 	loss_kl -0.5240064263 	acc 0.5171428571 	auroc 0.5616840278 	loss_nll 12806269.0000000000 	loss_mse 0.0643531010 	loss 12806268.0000000000 	inference time 0.3953709602 	time: 5.4034s 	
55 train	 	loss_kl -0.5393751860 	acc 0.5171428571 	auroc 0.5616666667 	loss_nll 12803385.0000000000 	loss_mse 0.0643386096 	loss 12803384.0000000000 	inference time 0.3640270233 	time: 5.3600s 	
56 train	 	loss_kl -0.5529001355 	acc 0.5152380952 	auroc 0.5609837963 	loss_nll 12803932.0000000000 	loss_mse 0.0643413737 	loss 12803931.0000000000 	inference time 0.3968961239 	time: 5.3491s 	
57 train	 	loss_kl -0.5627038479 	acc 0.5161904762 	auroc 0.5616550926 	loss_nll 12803036.0000000000 	loss_mse 0.0643368587 	loss 12803035.0000000000 	inference time 0.3331129551 	time: 5.4005s 	
58 train	 	loss_kl -0.5690815449 	acc 0.5161904762 	auroc 0.5610648148 	loss_nll 12801901.0000000000 	loss_mse 0.0643311590 	loss 12801900.0000000000 	inference time 0.2365427017 	time: 5.3410s 	
59 train	 	loss_kl -0.5722554922 	acc 0.5161904762 	auroc 0.5606770833 	loss_nll 12800815.0000000000 	loss_mse 0.0643257052 	loss 12800814.0000000000 	inference time 0.3798496723 	time: 5.4985s 	
60 train	 	loss_kl -0.5747358799 	acc 0.5171428571 	auroc 0.5615219907 	loss_nll 12800589.0000000000 	loss_mse 0.0643245652 	loss 12800588.0000000000 	inference time 0.3167116642 	time: 5.3415s 	
61 train	 	loss_kl -0.5860770941 	acc 0.5190476190 	auroc 0.5617708333 	loss_nll 12799499.0000000000 	loss_mse 0.0643190891 	loss 12799498.0000000000 	inference time 0.3649775982 	time: 5.4323s 	
62 train	 	loss_kl -0.5981494784 	acc 0.5180952381 	auroc 0.5621932870 	loss_nll 12799606.0000000000 	loss_mse 0.0643196255 	loss 12799605.0000000000 	inference time 0.2373394966 	time: 5.3207s 	
63 train	 	loss_kl -0.6106840968 	acc 0.5171428571 	auroc 0.5629340278 	loss_nll 12797623.0000000000 	loss_mse 0.0643096566 	loss 12797622.0000000000 	inference time 0.4057385921 	time: 5.5016s 	
64 train	 	loss_kl -0.6146179438 	acc 0.5152380952 	auroc 0.5626504630 	loss_nll 12796497.0000000000 	loss_mse 0.0643040016 	loss 12796496.0000000000 	inference time 0.3649230003 	time: 5.4620s 	
65 train	 	loss_kl -0.6197354198 	acc 0.5161904762 	auroc 0.5631886574 	loss_nll 12796239.0000000000 	loss_mse 0.0643027052 	loss 12796238.0000000000 	inference time 0.2540731430 	time: 5.2761s 	
66 train	 	loss_kl -0.6089943051 	acc 0.5161904762 	auroc 0.5629976852 	loss_nll 12794735.0000000000 	loss_mse 0.0642951503 	loss 12794734.0000000000 	inference time 0.3741033077 	time: 5.4488s 	
67 train	 	loss_kl -0.6080031395 	acc 0.5180952381 	auroc 0.5645138889 	loss_nll 12793743.0000000000 	loss_mse 0.0642901585 	loss 12793742.0000000000 	inference time 0.3951721191 	time: 5.4606s 	
68 train	 	loss_kl -0.6090357900 	acc 0.5209523810 	auroc 0.5646412037 	loss_nll 12793525.0000000000 	loss_mse 0.0642890707 	loss 12793524.0000000000 	inference time 0.3174419403 	time: 5.4632s 	
69 train	 	loss_kl -0.6386348605 	acc 0.5190476190 	auroc 0.5648842593 	loss_nll 12793295.0000000000 	loss_mse 0.0642879158 	loss 12793294.0000000000 	inference time 0.3811683655 	time: 5.4330s 	
70 train	 	loss_kl -0.6070842147 	acc 0.5228571429 	auroc 0.5625173611 	loss_nll 12790419.0000000000 	loss_mse 0.0642734617 	loss 12790418.0000000000 	inference time 0.3145918846 	time: 5.4461s 	
71 train	 	loss_kl -0.5887157321 	acc 0.5228571429 	auroc 0.5624016204 	loss_nll 12793111.0000000000 	loss_mse 0.0642869845 	loss 12793110.0000000000 	inference time 0.3317134380 	time: 5.3859s 	
72 train	 	loss_kl -0.5841077566 	acc 0.5228571429 	auroc 0.5627083333 	loss_nll 12791324.0000000000 	loss_mse 0.0642780066 	loss 12791323.0000000000 	inference time 0.3966252804 	time: 5.4001s 	
73 train	 	loss_kl -0.5861806273 	acc 0.5228571429 	auroc 0.5624768519 	loss_nll 12789559.0000000000 	loss_mse 0.0642691404 	loss 12789558.0000000000 	inference time 0.3636236191 	time: 5.4173s 	
74 train	 	loss_kl -0.5942308903 	acc 0.5228571429 	auroc 0.5623958333 	loss_nll 12789441.0000000000 	loss_mse 0.0642685443 	loss 12789440.0000000000 	inference time 0.3479409218 	time: 5.4021s 	
75 train	 	loss_kl -0.6257222891 	acc 0.5219047619 	auroc 0.5611689815 	loss_nll 12788121.0000000000 	loss_mse 0.0642619058 	loss 12788120.0000000000 	inference time 0.2860376835 	time: 5.3674s 	
76 train	 	loss_kl -0.6465758681 	acc 0.5171428571 	auroc 0.5616261574 	loss_nll 12787444.0000000000 	loss_mse 0.0642585084 	loss 12787443.0000000000 	inference time 0.3524718285 	time: 5.3693s 	
77 train	 	loss_kl -0.6458336711 	acc 0.5171428571 	auroc 0.5619733796 	loss_nll 12786862.0000000000 	loss_mse 0.0642555878 	loss 12786861.0000000000 	inference time 0.3041296005 	time: 5.3655s 	
78 train	 	loss_kl -0.6464699507 	acc 0.5180952381 	auroc 0.5608796296 	loss_nll 12785470.0000000000 	loss_mse 0.0642485991 	loss 12785469.0000000000 	inference time 0.2700564861 	time: 5.3330s 	
